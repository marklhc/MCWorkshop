---
title: "Advancing Quantitative Science"
subtitle: "with Monte Carlo Simulation <html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>"
author: "Hok Chio (Mark) Lai"
date: "2019/05/16"
output:
  xaringan::moon_reader:
    css: [slide-font.css, metropolis-fonts, default, metropolis]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r, load_refs, echo=FALSE, cache=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE, 
           bib.style = "authoryear", 
           cite.style = 'authoryear', 
           style = "markdown",
           hyperlink = FALSE, 
           dashed = FALSE)
myBib <- ReadBib("./references.bib", check = FALSE)
```

# Monte Carlo Methods

.pull-left[
- 1930s-1940s: Nuclear physics
   * Key figures:
       * Stanislaw Ulam
       * John von Neumann
       * Nicholas Metropolis
   * Manhattan project: hydrogen bomb
- Naming: Casino in Monaco
]

.pull-right[
![](images/Casino_Monte_Carlo.png)
]

---

# Why Do We Do Statistics?

- To study some target quantities in the population
    * Based on a limited sample
- How do we know that a statistics/statistical method gets us to a reasonable answer?
    * Analytic reasoning
    * Simulation

---
class: inverse, center, middle

# MC is one way to understand the properties of one or more statistical procedures

---

# What is MC (in Statistics)?

- Simulate the _process of repeated random sampling_
    * E.g., repeatedly drawing sample of IQ scores of size 10 from a population
- Approximate _sampling distributions_
    * Using __pseudorandom samples__
- Study properties of __estimators__
    * regression coefficients, fit index
    * compare multiple estimators or modeling approaches

---

# What is it (cont'd)?

- Based on Carsey & Harden (2014):
    * Simulations as experiments
        * Whether there's a "treatment" effect (but not why)
    * Simulations help develop intuition
        * Shouldn't replace analytically and theoretical reasoning 
    * Simulations help evaluate substantive theories and empirical results

???

Sometimes analytic solution does not exist

---

# Examples in the Literature

- [Curran, West, & Finch (1996, Psych Methods)](http://dx.doi.org/10.1037/1082-989X.1.1.16) studied the performance of the $\chi^2$ test for nonnormal data in CFA
- [Kim & Millsap (2014, MBR)](http://dx.doi.org/10.1080/00273171.2014.947352) studied the performance of the Bollen-Stine Bootstrapping method for evaluating SEM fit indices
- [MacCallum, Widaman, Zhang, & Hong (1999, Psych Methods)](http://dx.doi.org/10.1037/1082-989X.4.1.84) studied sample size requirement for getting stable EFA results
- [Maas & Hox (2005, Methodology)](http://dx.doi.org/10.1027/1614-2241.1.3.86) studied the sample size requirement for multilevel models

---

# Generating Random Data in R

With MC, one simulates the process of generating the data with an assumed 
__data generating model__

- Model: including both functional form and distributional assumptions

.font80[
```{r rnorm}
rnorm(5, mean = 0, sd = 1)
rnorm(5, mean = 0, sd = 1)  # numbers changed
```
]

---

# Setting the Seed

- Most programs use algorithms to generate numbers that look like random
    * _pseudorandom_
    * Completely determined by the _seed_
- For replicability, <font color="red">__ALWAYS__</font> explicitly set the seed
in the beginning

.font80[
```{r seed}
set.seed(1)
rnorm(5, mean = 0, sd = 1)
set.seed(1)
rnorm(5, mean = 0, sd = 1)  # same seed, same numbers
```
]

---

# Generating Data From Univariate Distributions

```{r, eval=FALSE}
rnorm(n, mean, sd)      # Normal distribution (mean and SD)
runif(n, min, max)      # Uniform distribution (minimum and maximum)
rchisq(n, df)           # Chi-squared distribution (degrees of freedom)
rbinom(n, size, prob)   # Binomial distribution
```

???

Other distributions include `exponential`, `gamma`, `beta`, `t`, `F`

---

# MC Approximation of $\mathcal{N}(0, 1)$

.small[
```{r, message=FALSE,fig.width=4, fig.height=3.5}
library(tidyverse)
set.seed(123)
nsim <- 20  # 20 samples
sam <- rnorm(nsim)  # default is mean = 0 and sd = 1
ggplot(tibble(x = sam), aes(x = x)) + 
  geom_density(bw = "SJ") + 
  stat_function(fun = dnorm, col = "red")  # overlay normal curve in red
```
]

---
class: inverse

# Exercise

Try increasing `nsim` to 100, then 1,000

---

# Exercise 

```{r, results='hide', echo=FALSE, animation.hook='ffmpeg', ffmpeg.format='gif', interval=0.2, dev='jpeg', message=FALSE}
set.seed(123)
nsim <- 1000  # 20 samples
sam <- rnorm(nsim)  # default is mean = 0 and sd = 1
plist <- vector("list", nsim / 10)
for (i in seq_along(plist)) {
  sam_t <- sam[1:(i * 10)]
  plist[[i]] <- ggplot(tibble(x = sam_t), aes(x = x)) +
    geom_histogram(aes(y = ..density..)) +
    stat_function(fun = dnorm, col = "red") +  # overlay normal curve in red
    annotate("text", label = paste("italic(n) == ", i * 10), 
             parse = TRUE, x = Inf, y = Inf, vjust = 1, hjust = 1)
}
plist
```


---

# Parameter vs Estimator

- __Estimator__/statistic: $T(\mathbf{X})$, or simply $T$
    * How good does it estimate the population parameter, $\theta$?
- Examples:
    * $T = \bar{X}$ estimates $\theta = \mu$
    * $T = \dfrac{\sum_i (X_i - \bar{X})^2}{N - 1}$ estimates $\theta = \sigma^2$

---

# Properties of Estimators

- Bias
- Consistency
- Efficiency
- Robustness

---

# What is a Good Estimator?

![](images/bias&efficiency.png)

---

# Sampling Distribution

- What is it?

```{r, echo=FALSE, message=FALSE, fig.width=4, fig.height=4}
NREP <- 1e4  # number of replications
sample_size <- 10  # define sample size
# Initialize place holders for results
sam_means <- rep(NA, NREP)  # an empty vector with NREP elements
for (i in seq_len(NREP)) {
  sam_means[i] <- mean(rchisq(sample_size, df = 4))
}
ggplot(data = tibble(xbar = sam_means), 
       aes(x = xbar)) + 
  geom_histogram()
```


---
class: inverse, center, middle

# Example I

Simulating Means and Medians

---

# When to use MC?

- When it's difficult to analytically derive the sampling distribution
    * E.g., indirect effect, fit-indexes; Cohen's $d$, *SE*s of estimators
- When required assumptions are violated
    * E.g., normality, large sample
    * Model is misspecified
    * Used to check __robustness__ of the estimator

---

# A Simulation Study is an Experiment

Experiment | Simulation
-----------|------------
Independent variables | Design factors 
Experimental conditions | Simulation conditions
Controlled variables | Other parameters
Procedure/Manipulation | Data generating model
Dependent variables | Evaluation criteria 
Substantive theory | Statistical theory
Participants | Replications

---
class: clear, center

```{r, echo=FALSE, out.width='65%'}
knitr::include_graphics("images/Sigal&Chalmers_figure1.png")
```

`r Citep(myBib, "Sigal2016", after = ", Figure 1, p. 141")`

---

# Design

Like experimental designs, conditions should be carefully chosen
- What to manipulate? Sample size? Effect size? Why?
    * Based on statistical theory and reasoning
    * E.g., Gauss-Markov theorem: regression coefficients are unbiased with 
    violations of distributional assumptions
- What levels? Why?
    * Needs to be realistic for empirical research
    * Maybe based on previous systematic reviews, 
    * Or a small review of your own
    
---

# Design (cont'd)

Full Factorial designs are most commonly used

Other alternatives include fractional factorial, random levels, etc
- See `r Citet(myBib, "skrondal2000")` for why they should be used more often

---

# Generate

- Starts with a statistical data generating model
    * E.g., $Y_i = \beta_0 + \beta_1 X_i + e_i,\quad e_i \overset{\textrm{i.i.d.}}{\sim} N(0, \sigma^2)$
        + Systematic (deterministic) component: $X_i$
        + Random (stochastic) component: $e_i$
        + Constants (parameters): $\beta_0$, $\beta_1$
    * $Y_i$ completely determined by $X_i, e_i, \beta_0, \beta_1$
    
```{r, echo=FALSE, fig.height=3.5}
DiagrammeR::grViz("
digraph reg {

  # a 'graph' statement
  graph [overlap = true, fontsize = 10]

  # several 'node' statements
  node [shape = box,
        fontname = Helvetica]
  X; Y

  node [shape = circle,
        fixedsize = true] // sets as circles
  e

  # several 'edge' statements
  X -> Y
  e -> Y
  {rank = same; X; Y;}
}
")
```


---

# Model-Based Simulation

```{r, echo=FALSE, out.width='75%'}
knitr::include_graphics("images/sampdist2.png")
```

---

# Analyze

Analyze the simulated data using one or more analytic approaches
- Misspecification: study the impact when analytic model omits important aspects of data generating model
    * E.g., ignoring clustering
- Comparison of approaches
    * E.g., Maximum likelihood vs. multiple imputation for missing data handling

---

# Summarize (Evaluation Criteria)

- $\bar{\hat \theta}$ = $\sum_{i = 1}^R \hat \theta_i / R$
- $\hat{\mathit{SD}}(\hat \theta)$ = 
  $\sqrt{\frac{\sum_{i = 1}^R (\theta_i - \bar{\hat \theta})^2}{R}}$

For evaluating estimators:

- Bias
    * Raw: $\bar{\hat \theta} - \theta$
    * Relative: Bias / $\theta$
    * Standardized: Bias / $\hat{\mathit{SD}}(\hat \theta)$
- Relative efficiency (only for unbiased estimators)
    * $\mathrm{RE}(\hat \theta, \tilde \theta)$ = $\frac{\hat{\mathrm{Var}}(\tilde \theta)}{\hat{\mathrm{Var}}(\hat \theta)}$

---

# Evaluation Criteria (cont'd)

For uncertainty estimators

- SE bias
    * Raw: $\overline{\mathit{SE}(\hat \theta)} - \hat{\mathit{SD}}(\hat \theta)$
    * Relative: SE bias / $\hat{\mathit{SD}}(\hat \theta)$

Combining bias and efficiency

- Mean squared error (MSE): $\frac{\sum_{i = 1}^R (\theta_i - \theta)^2}{R}$
    * Also = $\mathrm{Bias}^2 + \hat{\mathrm{Var}}(\hat \theta)$
    * Root MSE (RMSE) = $\sqrt{\mathrm{MSE}}$
- Mainly to compare 2+ estimators

---

# Evaluation Criteria (cont'd)

For statistical inferences:

- Power/Empirical Type I error rates
    * % with $p < \alpha$ (usually $\alpha$ = .05)
- Coverage of $C$% CI (e.g., $C$ = 95%)
    * % where the sample CI contains $\theta$

---
class: clear

.font80[
Criterion | Cutoff | Citation
-----|-------|----------
Bias |.|.
Relative bias | ≤5% | `r Citet(myBib, "hoogland1998")`
Standardized bias | ≤.40 | `r Citet(myBib, "collins2001")`
*SE* bias |.|.
Relative *SE* bias | ≤10% | `r Citet(myBib, "hoogland1998")`
MSE |.|.
RMSE |.|.
Empirical Type I error (α = .05) | 2.5% - 7.5% | `r Citet(myBib, "bradley1978")`
Power |.|.
95% CI Coverage | 91%-98% | `r Citet(myBib, "muthen2002")`
]

---

# Results

Just like you're analyzing real data

- Plots, figures
- ANOVA, regression
    * E.g., 3 (sample size) × 4 (parameter values) 2 (models) design: 2 between
    factors and 1 within factor

---
class: inverse, center, middle

# Example II

Simulation Example on Structural Equation Modeling

---

# Number of Replications

Should be justified rather than relying on rule of thumbs

## Why Does MC Work?

- Law of large number
    * $\sum_{i = 1}^R T_i / R \overset{p}{\to} \theta$
- When $R$ is large, 
    + the empirical distribution $\hat{F}(t)$ converges to the true sampling distribution $F(t)$. 

---

# Number of Replications (cont'd)

## How Good is the Approximation

- Monte Carlo (MC) Error 
    * Like standard error (SE) for a point estimate
- For expectations (e.g., bias)
    * MC Error = $\hat{\mathit{SD}}(\hat \theta) / \sqrt{R}$

E.g., if one wants the MC error to be ≤2.5% of the sampling variability, 
_R_ needs to be 1 / $.025^2$ = 1,600

---

# Number of Replications (cont'd)

For power (also Type I error) and CI coverage, 
* MC Error = $\sqrt{\frac{p (1 - p)}{R}}$

E.g., with _R_ = 250, and empirical Type I error = 5%, 
```{r}
sqrt((.05 * (1 - .05)) / 250)
```
So _R_ should be increase for more precise estimates

---

# Reporting MC Results

![](images/Boomsma_table1.png)

`r Citep(myBib, "boomsma2013", after = ", Table 1, p. 521")`

See `r Citet(myBib, "boomsma2013")`, Table 2, p. 526 for a checklist

---

# Efficiency tips

- Things that don't change should be outside of a loop
- Initialize place holders when using for-loops
- [Vectorize](https://adv-r.hadley.nz/perf-improve.html#vectorise)
- Strip out unnecessary computations
- Parallel computing (using the `future` package)

---

# Other topics not covered

- Error handling
- Assessing convergence
- Debugging
- Interfacing with other software (e.g., Mplus, LISREL, HLM)

---

# Further Readings

`r Citet(myBib, "carsey2014")` for a gentle introduction

`r Citet(myBib, "SimDesign")` and `r Citet(myBib, "Sigal2016")` for using the R
package `SimDesign`

`r Citet(myBib, "Harwell2018")` for a review of design and reporting practices

`r Citet(myBib, "skrondal2000")`, `r Citet(myBib, "serlin2000")`, and 
`r Citet(myBib, "bandalos2013")` for additional topics

---

class: inverse, center, middle

# Thanks!

---

# References

.font70[
```{r, 'refs', results='asis', echo=FALSE}
PrintBibliography(myBib, start = 1, end = 5)
```
]

---

# References (cont'd)

.font70[
```{r, 'refs2', results='asis', echo=FALSE}
PrintBibliography(myBib, start = 6, end = 10)
```
]

---

# References (cont'd)

.font70[
```{r, 'refs3', results='asis', echo=FALSE}
PrintBibliography(myBib, start = 11, end = 12)
```
]