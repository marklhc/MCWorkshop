---
title: "Advancing Quantitative Science With Monte Carlo Simulation"
subtitle: "PsyPag & MSCP-Section Simulation Summer School <html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>"
author: "Hok Chio (Mark) Lai, Winnie Wing-Yee Tse, & Yichi Zhang"
date: "2021/06/16"
output:
  xaringan::moon_reader:
    css: [default, my-theme.css]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{css echo = FALSE, eval = FALSE}
# Hide slide number when opacity = 0
.remark-slide-number {
  opacity: 0; /* default: 0.5 */
}
```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(tidyverse)
theme_set(theme_bw())
```

```{r load-RefManageR, load_refs, echo = FALSE, cache = FALSE, message = FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           style = "markdown",
           hyperlink = FALSE,
           dashed = FALSE, 
           max.names = 1, 
           no.print.fields = c("url", "issn", "language", "urldate"),
           longnamesfirst = FALSE)
myBib <- ReadBib("./references.bib", check = FALSE)
```

# Overview

### What is Monte Carlo (MC) simulation?

### Simulating Data From a Normal Distribution

### Properties of Statistical Methods

### Monte Carlo Simulation Study/Experiment

---

# Monte Carlo Methods

.pull-left[

- 1930s-1940s: Nuclear physics ([the Manhattan Project](https://www.atomicheritage.org/history/computing-and-manhattan-project))

   * Key figures:
   
       * Stanislaw Ulam
       * John von Neumann
       * Nicholas Metropolis
   
   * Naming: Casino in Monaco

]

.pull-right[
![](https://upload.wikimedia.org/wikipedia/commons/3/36/Real_Monte_Carlo_Casino.jpg)
]

.footnote[

Image credit: sam garza from Los Angeles, USA, CC BY 2.0 <https://creativecommons.org/licenses/by/2.0>, via Wikimedia Commons

]

---

# Why Do We Do Statistics?

- To study some target quantity in the population

    * Based on a limited sample

--

- How do we know that a statistics/statistical method gets us to a reasonable answer?

    * Analytic method

    * Simulation

--

> ## MC is one way to understand the properties of one or more statistical procedures

---

# What is MC (in Statistics)?

### A statistical technique that uses (psuedo-random) sampling to get numerical results

--

- Simulate the _process of repeated random sampling_

    * E.g., repeatedly drawing sample of IQ scores of size 10 from a population

--

- Approximate _sampling distributions_
    
    * Using **pseudorandom samples**

--

- Study properties of **statistical methods**

    * regression coefficients, fit index

    * compare multiple estimators or modeling approaches

---
class: inverse, middle, center

# Simulating Random Data From a Normal Distribution

---

# Generating Random Data in R

With MC, one simulates the process of generating the data with an assumed 
**data generating model/mechanism**

<!-- - Model: including both functional form and distributional assumptions -->

```{r rnorm}
rnorm(5, mean = 0, sd = 1)
rnorm(5, mean = 0, sd = 1)  # numbers changed
```

---

# Setting the Seed

- Most programs use algorithms to generate numbers that look like random, i.e., _pseudorandom_

    * Completely determined by the **state** of the random number generator, which can be set by the seed

> For replicability, set the seed explicitly

---

```{r seed}
state1 <- .Random.seed  # state of RNG
rnorm(5, mean = 0, sd = 1)
set.seed(1)
state2 <- .Random.seed  # state of RNG changed
identical(state1, state2)
rnorm(5, mean = 0, sd = 1)
set.seed(1)
state3 <- .Random.seed  # state of RNG unchanged with the same seed
identical(state2, state3)
rnorm(5, mean = 0, sd = 1)  # same seed, same numbers
```

---

# Generating Data From Univariate Distributions

```{r, eval=FALSE}
rnorm(n, mean, sd)      # Normal distribution (mean and SD)
runif(n, min, max)      # Uniform distribution (minimum and maximum)
rchisq(n, df)           # Chi-squared distribution (degrees of freedom)
rbinom(n, size, prob)   # Binomial distribution
```

???

Other distributions include `exponential`, `gamma`, `beta`, `t`, `F`

---

# MC Approximation of $N(0, 1)$

```{r norm-approx-20, message = FALSE, fig.width = 4, fig.height = 3.5, fig.align = "center"}
library(tibble)
library(ggplot2)
set.seed(123)
nsim <- 20  # 20 samples
sam <- rnorm(nsim)  # default is mean = 0 and sd = 1
ggplot(tibble(x = sam), aes(x = x)) + 
  geom_density(bw = "SJ") + 
  stat_function(fun = dnorm, col = "red")  # overlay normal curve in red
```

---
class: inverse, middle, center

# Exercise

### Try increasing `nsim` to 100, then 1,000

---

# Exercise 

```{r animate-sim-norm, results = "hide", echo = FALSE, animation.hook = "ffmpeg", ffmpeg.format = "gif", interval = 0.2, dev = "jpeg", message = FALSE, fig.width = 4.5, fig.asp = 1, fig.retina = 2, fig.align = "center", out.width = "60%", cache = TRUE}
set.seed(123)
nsim <- 1000  # 20 samples
sam <- rnorm(nsim)  # default is mean = 0 and sd = 1
plist <- vector("list", nsim / 10)
for (i in seq_along(plist)) {
  sam_t <- sam[1:(i * 10)]
  plist[[i]] <- ggplot(tibble(x = sam_t), aes(x = x)) +
    geom_histogram(aes(y = ..density..)) +
    stat_function(fun = dnorm, col = "red") +  # overlay normal curve in red
    annotate("text", label = paste("italic(n) == ", i * 10), 
             parse = TRUE, x = Inf, y = Inf, vjust = 1, hjust = 1)
}
plist
```

---
class: inverse, middle, center

# Evaluating Properties of Statistical Methods

---

# Some Types of Methods Studied by Simulations

Adapted from Table 3 of `r Citet(myBib, "Morris2019")`

```{r tab-common-target, echo = FALSE}
knitr::kable(
  tibble(
    Task = c("Estimation", "Uncertainty", "Inference", "Model Selection"),
    `Statistical Method` = c("Estimator", "Standard error, confidence interval", 
                             "Hypothesis testing", "Model selection index"),
    Properties = c("Bias, efficiency, consistency",
                   "SE bias, coverage",
                   "Type I error rate, power",
                   "Correct model rate")
  )
)
```

--

One additional property: **Robustness**---resilience against outliers and assumption violations

---

# Estimation: Parameter vs Estimator

- **Estimator**/statistic: $T(\mathbf{X})$, or simply $T$

    * How good does it estimate the population parameter, $\theta$?
    
- Examples:
    
    * $T = \bar{X}$ estimates $\theta = \mu$
    
    * $T = \dfrac{\sum_i (X_i - \bar{X})^2}{N - 1}$ estimates $\theta = \sigma^2$

---

# What is a Good Estimator?

```{r, echo = FALSE, out.width = '75%', fig.align = 'center'}
knitr::include_graphics("images/bias&efficiency.png")
```

---

# Sampling Distribution

- What is it?

```{r samp-dist-chisq, echo = FALSE, message = FALSE, fig.width = 4.5, fig.asp = 1, fig.align = "center", out.width = "60%"}
NREP <- 1e4  # number of replications
sample_size <- 10  # define sample size
# Initialize place holders for results
sam_means <- rep(NA, NREP)  # an empty vector with NREP elements
for (i in seq_len(NREP)) {
  sam_means[i] <- mean(rchisq(sample_size, df = 4))
}
ggplot(data = tibble(xbar = sam_means), 
       aes(x = xbar)) + 
  geom_histogram() + 
  labs(x = expression(bar(italic(X))))
```


---
class: inverse, center, middle

# Example I

### Simulating Means and Medians

---
class: inverse, center, middle

# Monte Carlo Simulation Study

---

# Examples in the Literature

- [Curran, West, & Finch (1996, Psych Methods)](doi.org/10.1037/1082-989X.1.1.16) studied the performance of the $\chi^2$ test for nonnormal data in CFA

- [Kim & Millsap (2014, MBR)](doi.org/10.1080/00273171.2014.947352) studied the performance of the Bollen-Stine Bootstrapping method for evaluating SEM fit indices

- [MacCallum, Widaman, Zhang, & Hong (1999, Psych Methods)](doi.org/10.1037/1082-989X.4.1.84) studied sample size requirement for getting stable EFA results
- [Maas & Hox (2005, Methodology)](http://dx.doi.org/10.1027/1614-2241.1.3.86) studied the sample size requirement for multilevel models

---

# A Simulation Study is an Experiment

Experiment | Simulation
-----------|------------
Independent variables | Design factors 
Experimental conditions | Simulation conditions
Controlled variables | Other parameters
Procedure/Manipulation | Data generating model
Dependent variables | Evaluation measures
Substantive theory | Statistical theory
Participants | Replications

---

# Framework

`r Citep(myBib, c("Sigal2016", "Chalmers2020", "Morris2019"))`

- Research questions
    * *What is the effect of ignoring random slopes in a growth model?*

--
    
- Design
    * *3 (N = 50, 100, 200) $\times$ 2 (slope variance = 0.1, 0.5) design*
    * *Constant: 4 time points, maximum likelihood estimation, etc*
    * *500 replications*

--

- Date-generating model (fixed and random components)
    * *linear growth model with normally distributed errors*

---

# Framework (cont'd)

- Statistical methods
    1. *slope estimate and standard error under correctly specified latent growth model with lavaan*
    2. *slope estimate and standard error under misspecified model*

--

- Evaluative measures
    * *convergence, bias, SE bias, relative efficiency*

--

- Summary and reporting
    * *Table, plot*

---

# Design

Like experimental designs, conditions should be carefully chosen
- What to manipulate? Sample size? Effect size? Why?
    * Based on statistical theory and reasoning
    * E.g., Gauss-Markov theorem: regression coefficients are unbiased with 
    violations of distributional assumptions

--

- What levels? Why?
    * Needs to be realistic for empirical research
    * Maybe based on previous systematic reviews, 
    * Or a small review of your own

--

Full Factorial designs are most commonly used

- Fractional factorial may sometimes be beneficial `r Citep(myBib, "skrondal2000")`

---

# Data Generation

- Starts with a statistical data generating model
    * E.g., $Y_i = \beta_0 + \beta_1 X_i + e_i,\quad e_i \overset{\textrm{i.i.d.}}{\sim} N(0, \sigma^2)$
        + Systematic (deterministic) component: $X_i$
        + Random (stochastic) component: $e_i$
        + Constants (parameters): $\beta_0$, $\beta_1$
    * $Y_i$ completely determined by $X_i, e_i, \beta_0, \beta_1$
    
```{r, echo=FALSE, fig.height=3.5}
DiagrammeR::grViz("
digraph reg {

  # a 'graph' statement
  graph [overlap = true, fontsize = 10]

  # several 'node' statements
  node [shape = box,
        fontname = Helvetica]
  X; Y

  node [shape = circle,
        fixedsize = true] // sets as circles
  e

  # several 'edge' statements
  X -> Y
  e -> Y
  {rank = same; X; Y;}
}
")
```


---

# Model-Based Simulation

```{r, echo = FALSE, out.width = '75%', fig.align = 'center'}
knitr::include_graphics("images/sampdist2.png")
```

---

# Statistical Methods

- Analyze each simulated data set with one or more approaches/models

- Obtain statistics of interest (e.g., estimate, SE, CI, $p$ value)

---

# Evaluative Measures

Some definitions:

|      |                               |
| ---- | ----------------------------- |
| Mean estimate | $\bar{\hat \theta} = \sum_{i = 1}^R \hat \theta_i / R$ |
| Average estimated SE | $\bar{\hat{\mathrm{SE}}}(\hat \theta) = \sum_{i = 1}^R \hat{\mathrm{SE}}(\hat \theta_i) / R$ |
| Empirical SE | $\hat{\mathit{SD}}(\hat \theta)$ = $\sqrt{\frac{\sum_{i = 1}^R (\theta_i - \bar{\hat \theta})^2}{R}}$ |

---

For estimators

|      |                               |
| ---- | ----------------------------- |
| Raw bias | $\bar{\hat \theta} - \theta$ |
| Relative bias | Bias / $\theta$         |
| Standardized bias | Bias / $\hat{\mathit{SD}}(\hat \theta)$ |
| Relative efficiency (RE; for unbiased estimators) | $\mathrm{RE}(\hat \theta, \tilde \theta)$ = $\frac{\hat{\mathit{SD}}^2(\tilde \theta)}{\hat{\mathit{SD}}^2(\hat \theta)}$ |
| Mean squared error (MSE) | $\mathrm{Bias}^2 + \hat{\mathrm{Var}}(\hat \theta)$ |
| Root Mean squared error (RMSE) | $\sqrt{\mathrm{MSE}}$ |

---

For uncertainty

|      |                               |
| ---- | ----------------------------- |
| SE bias | $\bar{\mathit{SE}}(\hat \theta) - \hat{\mathit{SD}}(\hat \theta)$ |
| Relative SE bias | SE bias / $\hat{\mathit{SD}}(\hat \theta)$ |
| Coverage | proportion of sample CIs containing $\theta$ |

--

For statistical inferences:

|      |                               |
| ---- | ----------------------------- |
| Power/Empirical Type I error rates | proportion with $p < \alpha$ (usually $\alpha$ = .05) |

---

# Summary and Reporting

Same as analyzing real data

- Plots, figures

- ANOVA, regression
    * E.g., 3 (sample size) × 4 (parameter values) 2 (models) design: 2 between
    factors and 1 within factor

---
class: inverse, center, middle

# Example II

Simulation Example on Structural Equation Modeling

---

# Number of Replications

MC requires large number of replications. But how large?

- Monte Carlo (MC) Error 
    * Like standard error (SE) for a point estimate
- For expectations (e.g., bias)
    * MC Error = $\hat{\mathit{SD}}(\hat \theta) / \sqrt{R}$

E.g., if one wants the MC error to be ≤2.5% of the sampling variability, 
_R_ needs to be 1 / $.025^2$ = 1,600

--

For power/Type I error/CI coverage, 
* MC Error = $\sqrt{\frac{p (1 - p)}{R}}$

E.g., with _R_ = 250, and empirical Type I error = 5%, MC Error = `r round(sqrt((.05 * (1 - .05)) / 250) * 100, 2)`%

---

# Further Readings

`r Citet(myBib, c("carsey2014", "Morris2019"))` for a gentle introduction

`r Citet(myBib, "Chalmers2020")` and `r Citet(myBib, "Sigal2016")` for using the R
package `SimDesign`

`r Citet(myBib, "Harwell2018")` for a review of design and reporting practices

`r Citet(myBib, "skrondal2000")`, `r Citet(myBib, "serlin2000")`, and 
`r Citet(myBib, "bandalos2013")` for additional topics

---

class: inverse, center, middle

# Thanks!

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).

Contact:

Mark Lai (hokchiol@usc.edu)

Winnie Wing-Yee Tse (wingyeet@usc.edu)

Yichi Zhang (yzhang97@usc.edu)

---

# References

.font70[
```{r, 'refs', results='asis', echo=FALSE}
PrintBibliography(myBib, start = 1, end = 5)
```
]

---

# References (cont'd)

.font70[
```{r, 'refs2', results='asis', echo=FALSE}
PrintBibliography(myBib, start = 6, end = 8)
```
]
